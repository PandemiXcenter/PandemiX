{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Notebook for downloading all data\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.dates as mdates\r\n",
    "import pandas as pd\r\n",
    "from pandas import ExcelWriter\r\n",
    "from pandas import ExcelFile\r\n",
    "import numpy as np\r\n",
    "import requests\r\n",
    "from bs4 import BeautifulSoup \r\n",
    "#import urllib.request\r\n",
    "import zipfile\r\n",
    "import io\r\n",
    "import os\r\n",
    "import datetime as dt\r\n",
    "# import pycountry as pc\r\n",
    "import math\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Flag for whether all data should be updated, or only recent\r\n",
    "downloadAllData = False\r\n",
    "downloadAllData = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Make folders if they aren't already there\r\n",
    "os.system(\"mkdir ssi_dashboard_zipped\")\r\n",
    "os.system(\"mkdir ssi_dashboard\")\r\n",
    "os.system(\"mkdir ssi_vacc_zipped\")\r\n",
    "os.system(\"mkdir ssi_vacc\")\r\n",
    "os.system(\"mkdir ssi_data_zipped\")\r\n",
    "os.system(\"mkdir ssi_data\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Define paths\r\n",
    "ssidatapath = \"ssi_data\"\r\n",
    "currootdir = os.getcwd() +\"/\" + ssidatapath"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "prevDownloads = os.listdir(currootdir)\r\n",
    "mostRecent = prevDownloads[-1]\r\n",
    "print(mostRecent)\r\n",
    "# curDate = np.datetime64(mostRecent[-2:] + '-' + mostRecent[-5:-3] + '-' + mostRecent[-10:-6])\r\n",
    "mostRecentDate = np.datetime64(mostRecent[-10:])\r\n",
    "mostRecentDate \r\n",
    "print(mostRecentDate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SSI_data_2021-09-16\n",
      "2021-09-16\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# mostRecentDate = np.datetime64('2021-08-11')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SCRIPT NOT YET FINISHED!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overv√•gningsdata"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "get_data = True\r\n",
    "ssidatapath = \"ssi_data\"\r\n",
    "rootdir = os.getcwd() +\"/\" + ssidatapath\r\n",
    "\r\n",
    "def download_url(url, save_path, chunk_size=128):\r\n",
    "    r = requests.get(url, stream=True)\r\n",
    "    with open(save_path, 'wb') as fd:\r\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\r\n",
    "            fd.write(chunk)\r\n",
    "\r\n",
    "url = \"https://covid19.ssi.dk/overvagningsdata/download-fil-med-overvaagningdata\"\r\n",
    "page = requests.get(url)\r\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\r\n",
    "links = soup.find_all(\"a\", string=lambda text: \"data\" in str(text).lower())\r\n",
    "\r\n",
    "check_str = \"<a href=\\\"https://files.ssi\"\r\n",
    "for link in links[3:]: \r\n",
    "    # print('---')\r\n",
    "    #print(link)\r\n",
    "    if str(link)[:len(check_str)]!=check_str:\r\n",
    "        print(\"not a file; continues...\")\r\n",
    "        continue\r\n",
    "    # print(link)\r\n",
    "    file = link[\"href\"]\r\n",
    "    yearPos = file.find('2021')\r\n",
    "    \r\n",
    "    if yearPos == -1:\r\n",
    "        print(\"2021 not found in link; continues...\")\r\n",
    "        continue\r\n",
    "\r\n",
    "    curDate = file[yearPos:yearPos+4] + '-' + file[yearPos-2:yearPos] + '-' + file[yearPos-4:yearPos-2] \r\n",
    "    \r\n",
    "    # print(file)\r\n",
    "    \r\n",
    "    # Only download new data\r\n",
    "    curDatetime = np.datetime64(curDate)\r\n",
    "    if (curDatetime > mostRecentDate):\r\n",
    "        print(curDatetime)\r\n",
    "\r\n",
    "        filename = \"SSI_data_\" + curDate\r\n",
    "        zipped_save_path = ssidatapath + \"_zipped/\" + filename + \".zip\"\r\n",
    "        extracted_save_path = ssidatapath + \"/\" + filename\r\n",
    "\r\n",
    "        \r\n",
    "        try:\r\n",
    "            download_url(file, zipped_save_path)\r\n",
    "            with zipfile.ZipFile(zipped_save_path, 'r') as zipObj:\r\n",
    "                zipObj.extractall(extracted_save_path)\r\n",
    "        except: \r\n",
    "            print(file)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not a file; continues...\n",
      "not a file; continues...\n",
      "2021-09-20\n",
      "2021-09-17\n",
      "2021-09-16\n",
      "2021-09-15\n",
      "2021-09-14\n",
      "2021-09-13\n",
      "2021-09-10\n",
      "2021-09-09\n",
      "2021-09-08\n",
      "2021-09-07\n",
      "2021-09-06\n",
      "2021-09-03\n",
      "2021-09-02\n",
      "2021-09-01\n",
      "2021-08-31\n",
      "2021-08-30\n",
      "2021-08-27\n",
      "2021-08-26\n",
      "2021-08-25\n",
      "2021-08-24\n",
      "2021-08-23\n",
      "2021-08-20\n",
      "2021-08-19\n",
      "2021-08-18\n",
      "2021-08-17\n",
      "2021-08-16\n",
      "2021-08-13\n",
      "2021-08-12\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "2021 not found in link; continues...\n",
      "not a file; continues...\n",
      "not a file; continues...\n",
      "not a file; continues...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vaccinedata"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "get_data = True\r\n",
    "ssivaccpath = \"ssi_vacc\"\r\n",
    "rootdir = os.getcwd() +\"/\" + ssivaccpath\r\n",
    "\r\n",
    "def download_url(url, save_path, chunk_size=128):\r\n",
    "    r = requests.get(url, stream=True)\r\n",
    "    with open(save_path, 'wb') as fd:\r\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\r\n",
    "            fd.write(chunk)\r\n",
    "\r\n",
    "# def get_all_data():\r\n",
    "url = \"https://covid19.ssi.dk/overvagningsdata/download-fil-med-vaccinationsdata\"\r\n",
    "page = requests.get(url)\r\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\r\n",
    "links = soup.find_all(\"a\", string=lambda text: \"data\" in str(text).lower())\r\n",
    "# print(links)\r\n",
    "check_str = \"<a href=\\\"https://files.ssi\"\r\n",
    "for link in links[3:]: \r\n",
    "    # print('---')\r\n",
    "    #print(link)\r\n",
    "    if str(link)[:len(check_str)]!=check_str:\r\n",
    "        print(\"not a file; continues...\")\r\n",
    "        continue\r\n",
    "    # print(link)\r\n",
    "    file = link[\"href\"]\r\n",
    "    yearPos = file.find('2021')\r\n",
    "    \r\n",
    "    if yearPos == -1:\r\n",
    "        print(\"2021 not found in link; continues...\")\r\n",
    "        continue\r\n",
    "    # print(yearPos)\r\n",
    "    # print(file[yearPos-4:yearPos+4])\r\n",
    "    curDate = file[yearPos:yearPos+4] + '-' + file[yearPos-2:yearPos] + '-' + file[yearPos-4:yearPos-2] \r\n",
    "\r\n",
    "\r\n",
    "    # Only download new data\r\n",
    "    curDatetime = np.datetime64(curDate)\r\n",
    "    if (curDatetime > mostRecentDate):\r\n",
    "        print(curDatetime)\r\n",
    "\r\n",
    "\r\n",
    "        filename = \"SSI_vacc_\" + curDate\r\n",
    "        zipped_save_path = ssivaccpath + \"_zipped/\" + filename + \".zip\"\r\n",
    "        extracted_save_path = ssivaccpath + \"/\" + filename\r\n",
    "        \r\n",
    "        try:\r\n",
    "            download_url(file, zipped_save_path)\r\n",
    "            with zipfile.ZipFile(zipped_save_path, 'r') as zipObj:\r\n",
    "                zipObj.extractall(extracted_save_path)\r\n",
    "        except: \r\n",
    "            print(file)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "not a file; continues...\n",
      "not a file; continues...\n",
      "2021-09-20\n",
      "2021-09-19\n",
      "2021-09-18\n",
      "2021-09-17\n",
      "2021-09-16\n",
      "2021-09-15\n",
      "2021-09-14\n",
      "2021-09-13\n",
      "2021-09-12\n",
      "2021-09-11\n",
      "2021-09-10\n",
      "2021-09-09\n",
      "2021-09-08\n",
      "2021-09-07\n",
      "2021-09-06\n",
      "2021-09-05\n",
      "2021-09-04\n",
      "2021-09-03\n",
      "2021-09-02\n",
      "2021-09-01\n",
      "2021-08-31\n",
      "2021-08-30\n",
      "2021-08-29\n",
      "2021-08-28\n",
      "2021-08-27\n",
      "2021-08-26\n",
      "2021-08-25\n",
      "2021-08-24\n",
      "2021-08-23\n",
      "2021-08-22\n",
      "2021-08-21\n",
      "2021-08-20\n",
      "2021-08-19\n",
      "2021-08-18\n",
      "2021-08-17\n",
      "2021-08-16\n",
      "2021-08-15\n",
      "2021-08-14\n",
      "2021-08-13\n",
      "2021-08-12\n",
      "not a file; continues...\n",
      "not a file; continues...\n",
      "not a file; continues...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dashboard data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "get_data = True\r\n",
    "ssidashpath = \"ssi_dashboard\"\r\n",
    "rootdir = os.getcwd() +\"/\" + ssidashpath\r\n",
    "\r\n",
    "def download_url(url, save_path, chunk_size=128):\r\n",
    "    r = requests.get(url, stream=True)\r\n",
    "    with open(save_path, 'wb') as fd:\r\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\r\n",
    "            fd.write(chunk)\r\n",
    "\r\n",
    "url = \"https://covid19.ssi.dk/overvagningsdata/download-fil-med-overvaagningdata\"\r\n",
    "\r\n",
    "page = requests.get(url)\r\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\r\n",
    "# links = soup.find_all(\"a\", string=lambda text: \"data\" in str(text).lower())\r\n",
    "links = soup.find_all(\"a\", string=lambda text: \"dash\" in str(text).lower())\r\n",
    "check_str = \"<a href=\\\"https://files.ssi\"\r\n",
    "for link in links[1:]: \r\n",
    "    \r\n",
    "\r\n",
    "    if str(link)[:len(check_str)]!=check_str:\r\n",
    "        print(\"not a file; continues...\")\r\n",
    "        continue\r\n",
    "    file = link[\"href\"]\r\n",
    "    yearPos = file.find('2021')\r\n",
    "    \r\n",
    "    if yearPos == -1:\r\n",
    "        print(\"2021 not found in link; continues...\")\r\n",
    "        continue\r\n",
    "    # print(yearPos)\r\n",
    "    # print(file[yearPos-4:yearPos+4])\r\n",
    "    curDate = file[yearPos:yearPos+4] + '-' + file[yearPos-2:yearPos] + '-' + file[yearPos-4:yearPos-2] \r\n",
    "\r\n",
    "    # Only download new data\r\n",
    "    curDatetime = np.datetime64(curDate)\r\n",
    "    if (curDatetime > mostRecentDate):\r\n",
    "        print(curDatetime)\r\n",
    "\r\n",
    "        filename = \"SSI_dashboard_\" + curDate\r\n",
    "        zipped_save_path = ssidashpath + \"_zipped/\" + filename + \".zip\"\r\n",
    "        extracted_save_path = ssidashpath + \"/\" + filename\r\n",
    "        \r\n",
    "        try:\r\n",
    "            download_url(file, zipped_save_path)\r\n",
    "            with zipfile.ZipFile(zipped_save_path, 'r') as zipObj:\r\n",
    "                zipObj.extractall(extracted_save_path)\r\n",
    "        except: \r\n",
    "            print(file)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-09-20\n",
      "2021-09-17\n",
      "2021-09-16\n",
      "2021-09-15\n",
      "2021-09-14\n",
      "2021-09-13\n",
      "2021-09-10\n",
      "2021-09-09\n",
      "2021-09-08\n",
      "2021-09-07\n",
      "2021-09-06\n",
      "2021-09-03\n",
      "2021-09-02\n",
      "2021-09-01\n",
      "2021-08-31\n",
      "2021-08-30\n",
      "2021-08-27\n",
      "2021-08-26\n",
      "2021-08-25\n",
      "2021-08-24\n",
      "2021-08-23\n",
      "2021-08-20\n",
      "2021-08-19\n",
      "2021-08-18\n",
      "2021-08-17\n",
      "2021-08-16\n",
      "2021-08-13\n",
      "2021-08-12\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "ssidatapath = \"ssi_data\"\r\n",
    "rootdir = os.getcwd() +\"/\" + ssidatapath\r\n",
    "\r\n",
    "for subdir, dirs, files in os.walk(rootdir):\r\n",
    "    if not len(files) == 0:\r\n",
    "        latestdir = subdir\r\n",
    "        latestDate = pd.to_datetime(subdir[-10:])\r\n",
    "\r\n",
    "print(latestdir)\r\n",
    "print(latestDate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\rakrpe\\Documents\\PandemiX\\GithubRepos\\PandemiX\\DanskeData/ssi_data\\SSI_data_2021-09-20\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c72cdf52cb3d31f207bca538e9396da8706c0ecf4a3038fb1c2b317934170431"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('main': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}