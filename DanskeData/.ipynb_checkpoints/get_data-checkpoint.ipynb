{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Danish'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A copy of the top of Christians script to load the data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "#import urllib.request\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import datetime as dt\n",
    "import pycountry as pc\n",
    "import math\n",
    "\n",
    "#from pandas.plotting import register_matplotlib_converters\n",
    "#register_matplotlib_converters()\n",
    "\n",
    "import locale\n",
    "locale.setlocale(locale.LC_TIME,\"Danish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danish regional data:\n",
    "\n",
    "get_data = True\n",
    "ssidatapath = \"ssi_data\"\n",
    "rootdir = os.getcwd() +\"/\" + ssidatapath\n",
    "\n",
    "\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "\n",
    "def get_all_data():\n",
    "    url = \"https://covid19.ssi.dk/overvagningsdata/download-fil-med-overvaagningdata\"\n",
    "    #url = \"http://www.ssi.dk/covid19/overvagning/data/data-epidemiologisk(e)-rapport-\"\n",
    "    #old link#url = \"https://www.ssi.dk/sygdomme-beredskab-og-forskning/sygdomsovervaagning/c/covid19-overvaagning/arkiv-med-overvaagningsdata-for-covid19\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    links = soup.find_all(\"a\", string=lambda text: \"data\" in str(text).lower())\n",
    "    #print(links.index(\"<a href=\\\"http://www.ssi.dk/covid19data\\\" target=\\\"_blank\\\">www.ssi.dk/covid19data</a>\"))\n",
    "    #print(links)\n",
    "    check_str = \"<a href=\\\"https://files.ssi\"\n",
    "    for link in links[3:]: \n",
    "        #print(link)\n",
    "        if str(link)[:len(check_str)]!=check_str:\n",
    "            print(\"not a file; continues...\")\n",
    "            continue\n",
    "        #print(link)\n",
    "        file = link[\"href\"]\n",
    "        old_date = str(file).split(\"-\")[-2]\n",
    "        if len(old_date)!=8:\n",
    "            print(\"not a date; continues...\")\n",
    "            continue\n",
    "        new_date = old_date[4:] + \"-\" + old_date[2:4] + \"-\" + old_date[0:2]\n",
    "        filename = \"SSI_data_\" + new_date\n",
    "        zipped_save_path = ssidatapath + \"_zipped/\" + filename + \".zip\"\n",
    "        extracted_save_path = ssidatapath + \"/\" + filename\n",
    "        \n",
    "        download_url(file, zipped_save_path)\n",
    "        with zipfile.ZipFile(zipped_save_path, 'r') as zipObj:\n",
    "            zipObj.extractall(extracted_save_path)\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)+1):\n",
    "        yield start_date + pd.DateOffset(days=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not a file; continues...\n",
      "not a file; continues...\n",
      "not a date; continues...\n",
      "not a date; continues...\n",
      "not a date; continues...\n",
      "not a date; continues...\n",
      "not a date; continues...\n",
      "not a file; continues...\n",
      "not a file; continues...\n",
      "not a file; continues...\n",
      "Data is loaded\n"
     ]
    }
   ],
   "source": [
    "# start_dt = pd.to_datetime(\"2020-03-01\")\n",
    "# end_dt = pd.to_datetime(\"2020-05-01\")\n",
    "\n",
    "#for date in daterange(start_dt, end_dt):\n",
    "    #print(date)\n",
    "#    print(dt.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "if not os.path.exists('figs'):\n",
    "    os.makedirs('figs')\n",
    "\n",
    "if get_data:\n",
    "    os.system(\"mkdir ssi_data_zipped\")\n",
    "    os.system(\"mkdir ssi_data\")\n",
    "    get_all_data()\n",
    "\n",
    "dk_cases_by_age_df = pd.DataFrame()\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    if not len(files) == 0:\n",
    "        date = pd.to_datetime(subdir[-10:])\n",
    "        for file in files:\n",
    "            if file.lower() == \"cases_by_age.csv\":\n",
    "                cases_age = pd.read_csv(subdir + \"/\" + file, sep=\";\", decimal=\",\")\n",
    "                cases_age[\"Dato\"] = date\n",
    "                cases_age[\"Antal_bekræftede_COVID-19\"] = pd.to_numeric(cases_age[\"Antal_bekræftede_COVID-19\"].astype(str).apply(lambda x: x.replace('.','')))\n",
    "                cases_age[\"Antal_testede\"] = pd.to_numeric(cases_age[\"Antal_testede\"].astype(str).apply(lambda x: x.replace('.','')))\n",
    "                dk_cases_by_age_df = dk_cases_by_age_df.append(cases_age, ignore_index=True)\n",
    "dk_cases_by_age_df = dk_cases_by_age_df.sort_values(by=['Dato', \"Aldersgruppe\"]).reset_index(drop=True)\n",
    "dk_cases_by_age_df#.columns\n",
    "testede_alder = dk_cases_by_age_df.groupby([\"Dato\", \"Aldersgruppe\"]).sum()[\"Antal_testede\"].unstack()\n",
    "bekr_alder = dk_cases_by_age_df.groupby([\"Dato\", \"Aldersgruppe\"]).sum()[\"Antal_bekræftede_COVID-19\"].unstack()\n",
    "\n",
    "# uncomment for daily data:\n",
    "testede_alder = testede_alder.diff()\n",
    "bekr_alder = bekr_alder.diff()\n",
    "\n",
    "if get_data:\n",
    "    print(\"Data is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading all data\n"
     ]
    }
   ],
   "source": [
    "print('Done loading all data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
